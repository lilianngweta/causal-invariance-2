{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHSIC approach for disentangling content and style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution - column 1 is class labels - column 0 is domain/environment labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">7.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2\n",
       "1   0        \n",
       "0.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "1.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "2.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "3.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "4.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "5.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "6.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "7.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "8.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "9.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/mctorch/nn/manifolds/stiefel.py:50: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1980.)\n",
      "  q, r = torch.qr(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.6990, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.4178, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.0847, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.0952, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.2505, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3239, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3639, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3757, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3883, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3917, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4039, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4575, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5081, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5719, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5880, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5905, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6099, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.6254, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  2\n",
      "loss:  tensor(-0.6334, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  3\n",
      "loss:  tensor(-0.6384, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  4\n",
      "loss:  tensor(-0.6423, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  5\n",
      "loss:  tensor(-0.6457, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  6\n",
      "loss:  tensor(-0.6483, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  7\n",
      "loss:  tensor(-0.6500, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  8\n",
      "loss:  tensor(-0.6511, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  9\n",
      "loss:  tensor(-0.6517, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  10\n",
      "loss:  tensor(-0.6520, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  11\n",
      "loss:  tensor(-0.6521, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  12\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  86\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  87\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  88\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  89\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  90\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  91\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  92\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  93\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  94\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  95\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  96\n",
      "loss:  tensor(-0.6522, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  97\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  98\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  99\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  100\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  101\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  102\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  103\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  104\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  105\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  106\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  107\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  108\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  109\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  110\n",
      "loss:  tensor(-0.6523, grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7d 23h 43min 19s, sys: 8h 7min 8s, total: 8d 7h 50min 28s\n",
      "Wall time: 5h 32min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8952,\n",
       " 'HSIC indist accuracy - spurious corr: ': 0.8602,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1079,\n",
       " 'HSIC ood accuracy- spurious corr: ': 0.086,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8731,\n",
       " 'HSIC indist accuracy - no spurious corr: ': 0.8162,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6833,\n",
       " 'HSIC ood accuracy - no spurious corr: ': 0.4691}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = 1.0\n",
    "alpha_sk = 0.5 # for creating skewed data used to learn R\n",
    "eta = 0.95\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "ns = 20 #specify number of style features\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import linalg as LA\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import mctorch.nn as mnn\n",
    "import mctorch.optim as moptim\n",
    "from hsic_calculator import HSIC, normalized_HSIC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations  \n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    style_labels = np.concatenate((np.zeros(len(modified_images)), np.ones(len(unmodified_images))), axis=None)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels, style_labels.astype(int)\n",
    "    \n",
    "\n",
    "# call this function to get experiments results for different parameters    \n",
    "def get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='simclr', transf_type='contrasted', \n",
    "                    dataset='cifar10', eta=0.95):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load saved image features\n",
    "    z_train_og = load('./data/Z_train_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_train_t = load('./data/Z_train_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    z_test_og = load('./data/Z_test_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_test_t = load('./data/Z_test_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    y_train_og = load('./data/train_labels_'+dataset+'.npy')\n",
    "\n",
    "    y_test_og = load('./data/test_labels_'+dataset+'.npy')\n",
    "    \n",
    "    # Create spurious correlations on train and test sets\n",
    "    z_train, train_labels, _ = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_indist, indist_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_ood, ood_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= 1-alpha, binarize_label=False)\n",
    "   \n",
    "    # concatenate original and transformed features\n",
    "    z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "    t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "    z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "    t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "   \n",
    "    # Prediction Accuracies on image features extracted using a baseline model\n",
    "    logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                                  random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "    baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "    baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "    baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)\n",
    "    \n",
    "    # Trained on original baseline features, tested on transformed features - no spurious correlations here\n",
    "    logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                     random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "    baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "    baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "    baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "          \n",
    "    # Obtain prediction coefficients of transformations done on images\n",
    "    z_train_rotated = load('./data/Z_train_rotated_cifar10_'+extractor+'.npy')\n",
    "    z_train_contrasted = load('./data/Z_train_contrasted_cifar10_'+extractor+'.npy')\n",
    "    z_train_blurred = load('./data/Z_train_blurred_cifar10_'+extractor+'.npy')\n",
    "    z_train_saturated = load('./data/Z_train_saturated_cifar10_'+extractor+'.npy')\n",
    "       \n",
    "\n",
    "    # Find R, get post-processed features, and perform predictions\n",
    "    \n",
    "    z_train_og_4_ts = np.concatenate((z_train_og, z_train_rotated,z_train_contrasted, \n",
    "                                      z_train_blurred,z_train_saturated), axis=0)\n",
    "\n",
    "    og_4_ts_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_rotated)), \n",
    "                                     np.array([2]*len(z_train_contrasted)), np.array([3]*len(z_train_blurred)), \n",
    "                                     np.array([4]*len(z_train_saturated))), axis=None)\n",
    "    \n",
    "    \n",
    "    image_labels = np.concatenate((y_train_og, y_train_og, y_train_og, y_train_og, y_train_og), axis=None)\n",
    "    \n",
    "    # concatenate features with sytle labels..style labels are in column 0\n",
    "    og_4_ts_labels_z_train_og_4_ts = np.concatenate((og_4_ts_labels.reshape(-1,1), image_labels.reshape(-1,1), z_train_og_4_ts), axis=1)\n",
    "\n",
    "    # shuffle data in t_labels_z_train_og_t\n",
    "    np.random.shuffle(og_4_ts_labels_z_train_og_4_ts)\n",
    "\n",
    "    shuffled_train_og_t = og_4_ts_labels_z_train_og_4_ts[:,2:]\n",
    "    shuffled_t_train_labels = og_4_ts_labels_z_train_og_4_ts[:,:1]\n",
    "    \n",
    "    # class distribution \n",
    "    labels_and_z_train_df = pd.DataFrame(og_4_ts_labels_z_train_og_4_ts)\n",
    "\n",
    "    print(\"class distribution - column 1 is class labels - column 0 is domain/environment labels\")\n",
    "    class_distribution_per_domain = labels_and_z_train_df.groupby([1,0]).count().iloc[:,0:1]\n",
    "    display(class_distribution_per_domain)\n",
    "    \n",
    "    \n",
    "    dtype = torch.FloatTensor\n",
    "    n = shuffled_train_og_t.shape[0]\n",
    "    d = shuffled_train_og_t.shape[1]\n",
    "    k = int(shuffled_train_og_t.shape[1]*eta) # % of original number of features\n",
    "\n",
    "    # Initialize R\n",
    "    R = mnn.Parameter(manifold=mnn.Stiefel(d,k)).float()\n",
    "\n",
    "    # print(\"Initial R\")\n",
    "    # display(R)\n",
    "\n",
    "    # Define Objective function \n",
    "    def obj(z, e, W, n_s=1):\n",
    "        z = torch.from_numpy(z).float()\n",
    "        e = torch.from_numpy(e).float()\n",
    "        MI_content_style = normalized_HSIC(torch.matmul(z, W[:,:n_s]), torch.matmul(z, W[:,n_s:]))\n",
    "        MI_conten_env = normalized_HSIC(torch.matmul(z,W[:,n_s:]), e)\n",
    "        MI_style_env = normalized_HSIC(torch.matmul(z,W[:,:n_s]), e)\n",
    "        loss = (MI_content_style + MI_conten_env) - MI_style_env\n",
    "        return loss\n",
    "\n",
    "    # Optimize - passing data in mini-batches\n",
    "    optimizer = moptim.rAdagrad(params = [R], lr=1e-2)\n",
    "\n",
    "    best_loss = 1e5\n",
    "    checkpoint = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        for index in range(0, len(shuffled_train_og_t), batch_size):\n",
    "            train_data_subset = shuffled_train_og_t[index:index+batch_size]\n",
    "            style_labels_subset = shuffled_t_train_labels[index:index+batch_size]\n",
    "            loss = obj(train_data_subset, style_labels_subset, R, ns)        \n",
    "            # saving R with the smallest loss value so far\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                print(\"Saving R, at epoch \", epoch)\n",
    "                checkpoint = {'epoch': epoch, 'loss': loss, 'R': R}\n",
    "                torch.save(checkpoint, 'checkpoint') \n",
    "                print(\"loss: \", loss)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # Load saved R\n",
    "    R_mat = torch.load('checkpoint')['R']\n",
    "\n",
    "    # Obtain post-processed features\n",
    "    f_train_og = z_train_og @ R_mat.detach().numpy()  \n",
    "    f_train = z_train @ R_mat.detach().numpy()\n",
    "    f_test_indist = z_test_indist @ R_mat.detach().numpy()\n",
    "    f_test_ood = z_test_ood @ R_mat.detach().numpy()\n",
    "    f_test_og = z_test_og @ R_mat.detach().numpy()\n",
    "    f_test_t = z_test_t @ R_mat.detach().numpy()\n",
    "    f_test_og_t = z_test_og_t @ R_mat.detach().numpy()\n",
    "\n",
    "    \n",
    "    # Correlation Matrix Analysis\n",
    "    if transf_type=='rotated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,1])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "            \n",
    "        \n",
    "    elif transf_type=='contrasted':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,2])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "    \n",
    "        \n",
    "    elif transf_type=='blurred':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,3])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "        \n",
    "    elif transf_type=='saturated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,4])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "\n",
    "    # Classification task using all post-processed features except style features    \n",
    "    lr_model_hsic_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                        random_state=0).fit(f_train[:,20:],train_labels)\n",
    "    hsic_sp_accuracy0 = lr_model_hsic_sp.score(f_train[:,20:], train_labels)\n",
    "    hsic_sp_accuracy1 = lr_model_hsic_sp.score(f_test_indist[:,20:], indist_test_labels)\n",
    "    hsic_sp_accuracy2 = lr_model_hsic_sp.score(f_test_ood[:,20:], ood_test_labels)\n",
    "    \n",
    "    # trained on original post-processed features, tested on transformed post-processed features \n",
    "    # without features without style features  \n",
    "    lr_model_hsic_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                        random_state=0).fit(f_train_og[:,20:],y_train_og)\n",
    "    hsic_no_sp_accuracy0 = lr_model_hsic_no_sp.score(f_train_og[:,20:], y_train_og)\n",
    "    hsic_no_sp_accuracy1 = lr_model_hsic_no_sp.score(f_test_og[:,20:], y_test_og)\n",
    "    hsic_no_sp_accuracy2 = lr_model_hsic_no_sp.score(f_test_t[:,20:], y_test_og)\n",
    "    \n",
    "    # put all the results in a dictionary\n",
    "    results_log = {}\n",
    "    results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "    results_log['HSIC indist accuracy - spurious corr: '] = hsic_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "    results_log['HSIC ood accuracy- spurious corr: '] = hsic_sp_accuracy2    \n",
    "\n",
    "    results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "    results_log['HSIC indist accuracy - no spurious corr: '] = hsic_no_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "    results_log['HSIC ood accuracy - no spurious corr: '] = hsic_no_sp_accuracy2 \n",
    "    \n",
    "    return results_log\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ITERS = range(10)\n",
    "#     datasets = ['cifar10'] \n",
    "#     extractors= ['resnet', 'simclr']  \n",
    "#     transf_types = ['contrasted', 'rotated', 'blurred', 'saturated']  \n",
    "#     alphas = [0.5,0.75,0.90,0.95,0.99,1.0] \n",
    "#     lamdas= [0,1,10,50]\n",
    "#     etas = [0.90,0.93,0.95,0.98,1.0]\n",
    "\n",
    "#     grid = list(product(datasets, extractors, transf_types, alphas, lamdas,etas,ITERS))\n",
    "    \n",
    "#     i = int(float(sys.argv[1]))\n",
    "#     dataset, extractor, transf_type, alpha, lamda, eta, ITER = grid[i]    \n",
    "\n",
    "#     results_log = get_exp_results(alpha = alpha, seed=int(ITER), lamda=lamda, extractor=extractor, \n",
    "#                                   transf_type=transf_type, dataset=dataset, eta=eta)\n",
    "    \n",
    "#     with open(f'summary_cifar10/summary_{i}.json', 'w') as fp:\n",
    "#         json.dump(results_log, fp)\n",
    "\n",
    "\n",
    "get_exp_results(alpha = 1.0, seed=0, lamda=10, extractor='resnet', transf_type='rotated', dataset='cifar10', eta=0.95)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with PISCO Results on CIFAR 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8952,\n",
       " 'PISCO indist accuracy - spurious corr: ': 0.8368,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1079,\n",
       " 'PISCO ood accuracy- spurious corr: ': 0.6547,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8731,\n",
       " 'PISCO indist accuracy - no spurious corr: ': 0.8433,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6833,\n",
       " 'PISCO ood accuracy - no spurious corr: ': 0.7299}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import linalg as LA\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations\n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels \n",
    "    \n",
    "\n",
    "# call this function to get experiments results for different parameters    \n",
    "def get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='simclr', transf_type='contrasted', \n",
    "                    dataset='cifar10', eta=0.95):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load saved image features\n",
    "    z_train_og = load('./data/Z_train_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_train_t = load('./data/Z_train_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    z_test_og = load('./data/Z_test_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_test_t = load('./data/Z_test_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    y_train_og = load('./data/train_labels_'+dataset+'.npy')\n",
    "\n",
    "    y_test_og = load('./data/test_labels_'+dataset+'.npy')\n",
    "    \n",
    "    # Create spurious correlations on train and test sets\n",
    "    z_train, train_labels = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_indist, indist_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_ood, ood_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= 1-alpha, binarize_label=False)\n",
    "   \n",
    "    # concatenate original and transformed features\n",
    "    z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "    t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "    z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "    t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "   \n",
    "    # Prediction Accuracies on image features extracted using a baseline model\n",
    "    logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                                  random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "    baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "    baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "    baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)\n",
    "    \n",
    "    # Trained on original baseline features, tested on transformed features - no spurious correlations here\n",
    "    logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                     random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "    baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "    baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "    baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "          \n",
    "    # Obtain prediction coefficients of transformations done on images\n",
    "    z_train_rotated = load('./data/Z_train_rotated_cifar10_'+extractor+'.npy')\n",
    "    z_train_contrasted = load('./data/Z_train_contrasted_cifar10_'+extractor+'.npy')\n",
    "    z_train_blurred = load('./data/Z_train_blurred_cifar10_'+extractor+'.npy')\n",
    "    z_train_saturated = load('./data/Z_train_saturated_cifar10_'+extractor+'.npy')\n",
    "       \n",
    "    z_train_og_rotated = np.concatenate((z_train_og, z_train_rotated), axis=0)\n",
    "    rotat_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_rotated))), axis=None)\n",
    "    \n",
    "    z_train_og_contrasted = np.concatenate((z_train_og, z_train_contrasted), axis=0)\n",
    "    contrast_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_contrasted))), axis=None)\n",
    "    \n",
    "    z_train_og_blurred= np.concatenate((z_train_og, z_train_blurred), axis=0)\n",
    "    blur_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_blurred))), axis=None)\n",
    "    \n",
    "    z_train_og_saturated = np.concatenate((z_train_og, z_train_saturated), axis=0)\n",
    "    saturat_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_saturated))), axis=None)\n",
    "    \n",
    "       \n",
    "    lr_model_rotated = LogisticRegression(random_state=0).fit(z_train_og_rotated, rotat_train_labels)\n",
    "    rotat_coefficients = lr_model_rotated.coef_.reshape(-1,1)\n",
    "    theta_1 = rotat_coefficients / np.linalg.norm(rotat_coefficients)\n",
    "    \n",
    "    lr_model_contrasted = LogisticRegression(random_state=0).fit(z_train_og_contrasted, contrast_train_labels)\n",
    "    contrast_coefficients = lr_model_contrasted.coef_.reshape(-1,1)\n",
    "    theta_2 = contrast_coefficients / np.linalg.norm(contrast_coefficients)\n",
    "    \n",
    "    lr_model_blurred = LogisticRegression(random_state=0).fit(z_train_og_blurred, blur_train_labels)\n",
    "    blur_coefficients = lr_model_blurred.coef_.reshape(-1,1)\n",
    "    theta_3 = blur_coefficients / np.linalg.norm(blur_coefficients)\n",
    "    \n",
    "    lr_model_saturated = LogisticRegression(random_state=0).fit(z_train_og_saturated, saturat_train_labels)\n",
    "    saturat_coefficients = lr_model_saturated.coef_.reshape(-1,1)\n",
    "    theta_4 = saturat_coefficients / np.linalg.norm(saturat_coefficients)\n",
    "       \n",
    "\n",
    "    # Find P, get post-processed features, and perform predictions\n",
    "    delta_z_matrix1 = z_train_og - z_train_rotated \n",
    "    delta_z_matrix2 = z_train_og - z_train_contrasted\n",
    "    delta_z_matrix3 = z_train_og - z_train_blurred\n",
    "    delta_z_matrix4 = z_train_og - z_train_saturated\n",
    "    combined_delta_z_matrix = np.concatenate((delta_z_matrix1, delta_z_matrix2,delta_z_matrix3,\n",
    "                                              delta_z_matrix4), axis=0)\n",
    "    \n",
    "    z_train_og_4_ts = np.concatenate((z_train_og, z_train_rotated,z_train_contrasted, \n",
    "                                      z_train_blurred,z_train_saturated), axis=0)\n",
    "    \n",
    "    k = int(z_train_og_4_ts.shape[1]*eta) # % of original number of features\n",
    "    n = z_train_og_4_ts.shape[0]\n",
    "    n_delt =  combined_delta_z_matrix.shape[0]\n",
    "\n",
    "    \n",
    "    M = - z_train_og_4_ts.T @ z_train_og_4_ts/n + lamda * combined_delta_z_matrix.T @ combined_delta_z_matrix /n_delt \n",
    "    \n",
    "    # Perform SVD to get eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = LA.eigh(M)\n",
    "\n",
    "    P_2 = eigenvectors[:,:(k-4)]\n",
    "\n",
    "    P = np.concatenate((theta_1,theta_2,theta_3,theta_4,P_2), axis=1)\n",
    "    \n",
    "    # Obtain post-processed features\n",
    "    f_train_og = z_train_og @ P  \n",
    "    f_train = z_train @ P \n",
    "    f_test_indist = z_test_indist @ P \n",
    "    f_test_ood = z_test_ood @ P \n",
    "    f_test_og = z_test_og @ P \n",
    "    f_test_t = z_test_t @ P \n",
    "    f_test_og_t = z_test_og_t @ P \n",
    "    \n",
    "    # Correlation Matrix Analysis\n",
    "    if transf_type=='rotated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,1])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "            \n",
    "        \n",
    "    elif transf_type=='contrasted':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,2])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "    \n",
    "        \n",
    "    elif transf_type=='blurred':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,3])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "        \n",
    "    elif transf_type=='saturated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,4])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "\n",
    "    # Classification task using all post-processed features except style features    \n",
    "    lr_model_pisco_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                        random_state=0).fit(f_train[:,4:],train_labels)\n",
    "    pisco_sp_accuracy0 = lr_model_pisco_sp.score(f_train[:,4:], train_labels)\n",
    "    pisco_sp_accuracy1 = lr_model_pisco_sp.score(f_test_indist[:,4:], indist_test_labels)\n",
    "    pisco_sp_accuracy2 = lr_model_pisco_sp.score(f_test_ood[:,4:], ood_test_labels)\n",
    "    \n",
    "    # trained on original post-processed features, tested on transformed post-processed features \n",
    "    # without features without style features  \n",
    "    lr_model_pisco_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                        random_state=0).fit(f_train_og[:,4:],y_train_og)\n",
    "    pisco_no_sp_accuracy0 = lr_model_pisco_no_sp.score(f_train_og[:,4:], y_train_og)\n",
    "    pisco_no_sp_accuracy1 = lr_model_pisco_no_sp.score(f_test_og[:,4:], y_test_og)\n",
    "    pisco_no_sp_accuracy2 = lr_model_pisco_no_sp.score(f_test_t[:,4:], y_test_og)\n",
    "    \n",
    "    # put all the results in a dictionary\n",
    "    results_log = {}\n",
    "    results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "    results_log['PISCO indist accuracy - spurious corr: '] = pisco_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "    results_log['PISCO ood accuracy- spurious corr: '] = pisco_sp_accuracy2    \n",
    "\n",
    "    results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "    results_log['PISCO indist accuracy - no spurious corr: '] = pisco_no_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "    results_log['PISCO ood accuracy - no spurious corr: '] = pisco_no_sp_accuracy2 \n",
    "    \n",
    "    return results_log\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ITERS = range(10)\n",
    "#     datasets = ['cifar10'] \n",
    "#     extractors= ['resnet', 'simclr']  \n",
    "#     transf_types = ['contrasted', 'rotated', 'blurred', 'saturated']  \n",
    "#     alphas = [0.5,0.75,0.90,0.95,0.99,1.0] \n",
    "#     lamdas= [0,1,10,50]\n",
    "#     etas = [0.90,0.93,0.95,0.98,1.0]\n",
    "\n",
    "#     grid = list(product(datasets, extractors, transf_types, alphas, lamdas,etas,ITERS))\n",
    "    \n",
    "#     i = int(float(sys.argv[1]))\n",
    "#     dataset, extractor, transf_type, alpha, lamda, eta, ITER = grid[i]    \n",
    "\n",
    "#     results_log = get_exp_results(alpha = alpha, seed=int(ITER), lamda=lamda, extractor=extractor, \n",
    "#                                   transf_type=transf_type, dataset=dataset, eta=eta)\n",
    "    \n",
    "#     with open(f'summary_cifar10/summary_{i}.json', 'w') as fp:\n",
    "#         json.dump(results_log, fp)\n",
    "\n",
    "\n",
    "get_exp_results(alpha = 1.0, seed=0, lamda=10, extractor='resnet', transf_type='rotated', dataset='cifar10', eta=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:invariance_env]",
   "language": "python",
   "name": "conda-env-invariance_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
