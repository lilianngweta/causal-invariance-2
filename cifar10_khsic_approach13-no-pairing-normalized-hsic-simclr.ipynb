{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHSIC approach for disentangling content and style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.84 s, sys: 9.96 s, total: 16.8 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = 1.0\n",
    "alpha_sk = 0.5 # for creating skewed data used to learn R\n",
    "eta = 0.95\n",
    "batch_size = 128\n",
    "ns = 1 #specify number of style features\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations on \n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    style_labels = np.concatenate((np.zeros(len(modified_images)), np.ones(len(unmodified_images))), axis=None)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels, style_labels.astype(int)\n",
    "\n",
    "\n",
    "# Load saved image features\n",
    "\n",
    "z_train_og = load('./data/Z_train_og_cifar10_simclr.npy')\n",
    "z_train_t = load('./data/Z_train_rotated_cifar10_simclr.npy')\n",
    "\n",
    "z_test_og = load('./data/Z_test_og_cifar10_simclr.npy')\n",
    "z_test_t = load('./data/Z_test_rotated_cifar10_simclr.npy')\n",
    "\n",
    "y_train_og = load('./data/train_labels_cifar10.npy')\n",
    "\n",
    "y_test_og = load('./data/test_labels_cifar10.npy')\n",
    "\n",
    "\n",
    "# Create spurious correlations on train and test sets\n",
    "\n",
    "z_train_sk, train_labels_sk, t_labels_sk = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                         spu_corr= alpha_sk, binarize_label=False)\n",
    "\n",
    "z_train, train_labels, _ = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                         spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "z_test_indist, indist_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                         spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "z_test_ood, ood_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                         spu_corr= 1-alpha, binarize_label=False)\n",
    "\n",
    "\n",
    "\n",
    "# concatenate original and colored features\n",
    "z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "\n",
    "\n",
    "# # concatenate features with sytle labels..style labels are in column 0\n",
    "# t_labels_z_train_og_t = np.concatenate((t_train_labels.reshape(-1,1), z_train_og_t), axis=1)\n",
    "\n",
    "# # shuffle data in t_labels_z_train_og_t\n",
    "# np.random.shuffle(t_labels_z_train_og_t)\n",
    "\n",
    "# shuffled_train_og_t = t_labels_z_train_og_t[:,1:]\n",
    "# shuffled_t_train_labels = t_labels_z_train_og_t[:,:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data class distribustions per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution - column 1 is class labels - column 0 is domain/environment labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2\n",
       "1   0        \n",
       "0.0 0.0  2478\n",
       "    1.0  2522\n",
       "1.0 0.0  2469\n",
       "    1.0  2531\n",
       "2.0 0.0  2455\n",
       "    1.0  2545\n",
       "3.0 0.0  2447\n",
       "    1.0  2553\n",
       "4.0 0.0  2500\n",
       "    1.0  2500\n",
       "5.0 0.0  2526\n",
       "    1.0  2474\n",
       "6.0 0.0  2500\n",
       "    1.0  2500\n",
       "7.0 0.0  2524\n",
       "    1.0  2476\n",
       "8.0 0.0  2494\n",
       "    1.0  2506\n",
       "9.0 0.0  2432\n",
       "    1.0  2568"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# class distribution in original and colored images - class distribution is skewed\n",
    "style_and_img_labels_z_train_sk_df = pd.DataFrame(np.concatenate((t_labels_sk.reshape(-1,1),\n",
    "                                                                  train_labels_sk.reshape(-1,1),z_train_sk), axis=1))\n",
    "\n",
    "print(\"class distribution - column 1 is class labels - column 0 is domain/environment labels\")\n",
    "class_distribution_per_domain = style_and_img_labels_z_train_sk_df.groupby([1,0]).count().iloc[:,0:1]\n",
    "display(class_distribution_per_domain)\n",
    "\n",
    "\n",
    "# shuffle data in style_and_img_labels_z_train_sk_df\n",
    "style_and_img_labels_z_train_sk = style_and_img_labels_z_train_sk_df.to_numpy()\n",
    "np.random.shuffle(style_and_img_labels_z_train_sk)\n",
    "\n",
    "shuffled_train_og_t = style_and_img_labels_z_train_sk[:,2:]\n",
    "shuffled_t_train_labels = style_and_img_labels_z_train_sk[:,:1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find rotation matrix R by optimization----using KHSIC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/mctorch/nn/manifolds/stiefel.py:50: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1980.)\n",
      "  q, r = torch.qr(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.3852, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.2537, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.1567, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.1290, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.0723, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.0894, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1064, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3025, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3305, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3721, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4194, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4585, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6638, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6849, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7005, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7147, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7173, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7343, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7532, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.7972, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.8375, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.8449, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.8528, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.8672, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.8703, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.8727, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.8734, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  2\n",
      "loss:  tensor(-0.8881, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  2\n",
      "loss:  tensor(-0.8897, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  3\n",
      "loss:  tensor(-0.8913, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  3\n",
      "loss:  tensor(-0.8988, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  3\n",
      "loss:  tensor(-0.8992, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  4\n",
      "loss:  tensor(-0.9029, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  4\n",
      "loss:  tensor(-0.9053, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  5\n",
      "loss:  tensor(-0.9093, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  5\n",
      "loss:  tensor(-0.9094, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  5\n",
      "loss:  tensor(-0.9096, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  6\n",
      "loss:  tensor(-0.9131, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  6\n",
      "loss:  tensor(-0.9133, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  7\n",
      "loss:  tensor(-0.9154, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  7\n",
      "loss:  tensor(-0.9164, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  8\n",
      "loss:  tensor(-0.9169, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  8\n",
      "loss:  tensor(-0.9191, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  9\n",
      "loss:  tensor(-0.9213, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  10\n",
      "loss:  tensor(-0.9233, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  11\n",
      "loss:  tensor(-0.9250, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  12\n",
      "loss:  tensor(-0.9264, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  13\n",
      "loss:  tensor(-0.9277, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  14\n",
      "loss:  tensor(-0.9287, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  15\n",
      "loss:  tensor(-0.9296, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  16\n",
      "loss:  tensor(-0.9304, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  17\n",
      "loss:  tensor(-0.9310, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  18\n",
      "loss:  tensor(-0.9316, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  19\n",
      "loss:  tensor(-0.9320, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  20\n",
      "loss:  tensor(-0.9324, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  21\n",
      "loss:  tensor(-0.9327, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  22\n",
      "loss:  tensor(-0.9330, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  23\n",
      "loss:  tensor(-0.9332, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  24\n",
      "loss:  tensor(-0.9334, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  25\n",
      "loss:  tensor(-0.9336, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  26\n",
      "loss:  tensor(-0.9338, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  27\n",
      "loss:  tensor(-0.9339, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  28\n",
      "loss:  tensor(-0.9340, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  29\n",
      "loss:  tensor(-0.9341, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  30\n",
      "loss:  tensor(-0.9342, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  31\n",
      "loss:  tensor(-0.9343, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  32\n",
      "loss:  tensor(-0.9343, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  33\n",
      "loss:  tensor(-0.9344, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  34\n",
      "loss:  tensor(-0.9344, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  35\n",
      "loss:  tensor(-0.9344, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  36\n",
      "loss:  tensor(-0.9344, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  38\n",
      "loss:  tensor(-0.9346, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  39\n",
      "loss:  tensor(-0.9349, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  40\n",
      "loss:  tensor(-0.9352, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  41\n",
      "loss:  tensor(-0.9355, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  42\n",
      "loss:  tensor(-0.9358, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  43\n",
      "loss:  tensor(-0.9361, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  44\n",
      "loss:  tensor(-0.9364, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  45\n",
      "loss:  tensor(-0.9367, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  46\n",
      "loss:  tensor(-0.9369, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  47\n",
      "loss:  tensor(-0.9372, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  48\n",
      "loss:  tensor(-0.9374, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  49\n",
      "loss:  tensor(-0.9376, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  50\n",
      "loss:  tensor(-0.9378, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  51\n",
      "loss:  tensor(-0.9380, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  52\n",
      "loss:  tensor(-0.9382, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  53\n",
      "loss:  tensor(-0.9384, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  54\n",
      "loss:  tensor(-0.9384, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  54\n",
      "loss:  tensor(-0.9386, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  55\n",
      "loss:  tensor(-0.9387, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  55\n",
      "loss:  tensor(-0.9387, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  56\n",
      "loss:  tensor(-0.9389, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  57\n",
      "loss:  tensor(-0.9392, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  58\n",
      "loss:  tensor(-0.9394, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  59\n",
      "loss:  tensor(-0.9397, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  60\n",
      "loss:  tensor(-0.9399, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  61\n",
      "loss:  tensor(-0.9401, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  62\n",
      "loss:  tensor(-0.9403, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  63\n",
      "loss:  tensor(-0.9405, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  64\n",
      "loss:  tensor(-0.9407, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  65\n",
      "loss:  tensor(-0.9409, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  66\n",
      "loss:  tensor(-0.9410, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  67\n",
      "loss:  tensor(-0.9412, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  68\n",
      "loss:  tensor(-0.9414, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  69\n",
      "loss:  tensor(-0.9415, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  70\n",
      "loss:  tensor(-0.9416, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  71\n",
      "loss:  tensor(-0.9418, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  72\n",
      "loss:  tensor(-0.9419, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  73\n",
      "loss:  tensor(-0.9420, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  74\n",
      "loss:  tensor(-0.9421, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  75\n",
      "loss:  tensor(-0.9422, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  76\n",
      "loss:  tensor(-0.9423, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  77\n",
      "loss:  tensor(-0.9424, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  78\n",
      "loss:  tensor(-0.9425, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  79\n",
      "loss:  tensor(-0.9426, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  80\n",
      "loss:  tensor(-0.9427, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  81\n",
      "loss:  tensor(-0.9427, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  82\n",
      "loss:  tensor(-0.9428, grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving R, at epoch  83\n",
      "loss:  tensor(-0.9428, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  84\n",
      "loss:  tensor(-0.9429, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  85\n",
      "loss:  tensor(-0.9430, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  86\n",
      "loss:  tensor(-0.9430, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  87\n",
      "loss:  tensor(-0.9430, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  88\n",
      "loss:  tensor(-0.9431, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  89\n",
      "loss:  tensor(-0.9431, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  90\n",
      "loss:  tensor(-0.9431, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  91\n",
      "loss:  tensor(-0.9432, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  92\n",
      "loss:  tensor(-0.9432, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  93\n",
      "loss:  tensor(-0.9432, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  94\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  95\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  96\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  97\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  98\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  99\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  100\n",
      "loss:  tensor(-0.9433, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  101\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  102\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  103\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  104\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  105\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  106\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  107\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  108\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  109\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  110\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  111\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  112\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  113\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  114\n",
      "loss:  tensor(-0.9434, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  115\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  116\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  117\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  118\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  119\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  120\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  121\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  122\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  123\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  124\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  125\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  126\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  127\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  182\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  183\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  184\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  185\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  186\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  187\n",
      "loss:  tensor(-0.9435, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  188\n",
      "loss:  tensor(-0.9436, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  189\n",
      "loss:  tensor(-0.9436, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  190\n",
      "loss:  tensor(-0.9436, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  191\n",
      "loss:  tensor(-0.9436, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  192\n",
      "loss:  tensor(-0.9436, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  193\n",
      "loss:  tensor(-0.9437, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  194\n",
      "loss:  tensor(-0.9437, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  195\n",
      "loss:  tensor(-0.9437, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  196\n",
      "loss:  tensor(-0.9437, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  197\n",
      "loss:  tensor(-0.9438, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  198\n",
      "loss:  tensor(-0.9438, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  199\n",
      "loss:  tensor(-0.9438, grad_fn=<SubBackward0>)\n",
      "CPU times: user 1d 15h 39min 44s, sys: 2h 33min 37s, total: 1d 18h 13min 21s\n",
      "Wall time: 1h 24min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 199,\n",
       " 'loss': tensor(-0.9438, grad_fn=<SubBackward0>),\n",
       " 'R': Parameter containing:\n",
       " tensor([[ 0.0585, -0.0051, -0.0013,  ..., -0.0670, -0.0335,  0.0252],\n",
       "         [ 0.0199, -0.0271, -0.0200,  ...,  0.0200,  0.0286,  0.0428],\n",
       "         [-0.0763,  0.0617, -0.0804,  ...,  0.0485,  0.0287, -0.0919],\n",
       "         ...,\n",
       "         [ 0.0047,  0.0458, -0.0240,  ..., -0.0502,  0.0284, -0.0324],\n",
       "         [ 0.1188,  0.0250,  0.0311,  ..., -0.0002, -0.0710,  0.0432],\n",
       "         [-0.0208,  0.0348,  0.0785,  ..., -0.0016,  0.0422, -0.0278]],\n",
       "        requires_grad=True)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "import mctorch.nn as mnn\n",
    "import mctorch.optim as moptim\n",
    "from hsic_calculator import HSIC, normalized_HSIC\n",
    "\n",
    "\n",
    "# # Reduce the samples size\n",
    "# shuffled_train_og_t = shuffled_train_og_t[:10000]\n",
    "# shuffled_t_train_labels = shuffled_t_train_labels[:10000]\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "n = shuffled_train_og_t.shape[0]\n",
    "d = shuffled_train_og_t.shape[1]\n",
    "k = int(shuffled_train_og_t.shape[1]*eta) # % of original number of features\n",
    "\n",
    "\n",
    "# Initialize R\n",
    "R = mnn.Parameter(manifold=mnn.Stiefel(d,k)).float()\n",
    "\n",
    "# print(\"Initial R\")\n",
    "# display(R)\n",
    "\n",
    "# Define Objective function \n",
    "def obj(z, e, W, n_s=1):\n",
    "    z = torch.from_numpy(z).float()\n",
    "    e = torch.from_numpy(e).float()\n",
    "    MI_content_style = normalized_HSIC(torch.matmul(z, W[:,:n_s]), torch.matmul(z, W[:,n_s:]))\n",
    "    MI_conten_env = normalized_HSIC(torch.matmul(z,W[:,n_s:]), e)\n",
    "    MI_style_env = normalized_HSIC(torch.matmul(z,W[:,:n_s]), e)\n",
    "    loss = (MI_content_style + MI_conten_env) - MI_style_env\n",
    "    return loss\n",
    "\n",
    "# Optimize - passing data in mini-batches\n",
    "optimizer = moptim.rAdagrad(params = [R], lr=1e-2)\n",
    "\n",
    "best_loss = 1e5\n",
    "checkpoint = {}\n",
    "for epoch in range(200):\n",
    "    for index in range(0, len(shuffled_train_og_t), batch_size):\n",
    "        train_data_subset = shuffled_train_og_t[index:index+batch_size]\n",
    "        style_labels_subset = shuffled_t_train_labels[index:index+batch_size]\n",
    "        loss = obj(train_data_subset, style_labels_subset, R, ns)        \n",
    "        # saving R with the smallest loss value so far\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            print(\"Saving R, at epoch \", epoch)\n",
    "            checkpoint = {'epoch': epoch, 'loss': loss, 'R': R}\n",
    "            torch.save(checkpoint, 'checkpoint') \n",
    "            print(\"loss: \", loss)            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "checkpoint\n",
    "\n",
    "# print(\"R after optimization\")\n",
    "# display(R)\n",
    "# (R.T)@R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the obtained rotation matrix R to disentangle content and style for OOD generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 59s, sys: 34min 25s, total: 48min 24s\n",
      "Wall time: 46.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8477,\n",
       " 'HSIC Approach indist accuracy - spurious corr: ': 0.8221,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1579,\n",
       " 'HSIC Approach ood accuracy- spurious corr: ': 0.301,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8287,\n",
       " 'HSIC Approach indist accuracy - no spurious corr: ': 0.8115,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6206,\n",
       " 'HSIC Approach ood accuracy - no spurious corr: ': 0.4599}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load saved R\n",
    "R_mat = torch.load('checkpoint')['R']\n",
    "\n",
    "# Obtain post-processed features\n",
    "f_train_og = z_train_og @ R_mat.detach().numpy()  \n",
    "f_train = z_train @ R_mat.detach().numpy()\n",
    "f_test_indist = z_test_indist @ R_mat.detach().numpy()\n",
    "f_test_ood = z_test_ood @ R_mat.detach().numpy()\n",
    "f_test_og = z_test_og @ R_mat.detach().numpy()\n",
    "f_test_t = z_test_t @ R_mat.detach().numpy()\n",
    "f_test_og_t = z_test_og_t @ R_mat.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "########### Baseline results ###############\n",
    "# Prediction Accuracies on image features extracted using a baseline model (mlp)\n",
    "logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                              random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)   \n",
    "\n",
    "# Trained on original baseline features, tested on colored features - no spurious correlations here\n",
    "logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                 random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "####################################\n",
    "\n",
    "# Classification task using all post-processed features except style features   \n",
    "lr_model_new_HSIC_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                    random_state=0).fit(f_train[:,1:],train_labels)\n",
    "new_HSIC_sp_accuracy0 = lr_model_new_HSIC_sp.score(f_train[:,1:], train_labels)\n",
    "new_HSIC_sp_accuracy1 = lr_model_new_HSIC_sp.score(f_test_indist[:,1:], indist_test_labels)\n",
    "new_HSIC_sp_accuracy2 = lr_model_new_HSIC_sp.score(f_test_ood[:,1:], ood_test_labels)\n",
    "\n",
    "# trained on original post-processed features, tested on transformed post-processed \n",
    "# features without style features   \n",
    "lr_model_new_HSIC_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                    random_state=0).fit(f_train_og[:,1:],y_train_og)\n",
    "new_HSIC_no_sp_accuracy0 = lr_model_new_HSIC_no_sp.score(f_train_og[:,1:], y_train_og)\n",
    "new_HSIC_no_sp_accuracy1 = lr_model_new_HSIC_no_sp.score(f_test_og[:,1:], y_test_og)\n",
    "new_HSIC_no_sp_accuracy2 = lr_model_new_HSIC_no_sp.score(f_test_t[:,1:], y_test_og)\n",
    "\n",
    "# put all the results in a dictionary\n",
    "results_log = {}\n",
    "\n",
    "results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "results_log['HSIC Approach indist accuracy - spurious corr: '] = new_HSIC_sp_accuracy1\n",
    "\n",
    "results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "results_log['HSIC Approach ood accuracy- spurious corr: '] = new_HSIC_sp_accuracy2    \n",
    "\n",
    "results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "results_log['HSIC Approach indist accuracy - no spurious corr: '] = new_HSIC_no_sp_accuracy1\n",
    "\n",
    "results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "results_log['HSIC Approach ood accuracy - no spurious corr: '] = new_HSIC_no_sp_accuracy2   \n",
    "\n",
    "\n",
    "results_log\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with PISCO Results on CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 48s, sys: 40min 21s, total: 57min 9s\n",
      "Wall time: 56 s\n",
      "Parser   : 115 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8477,\n",
       " 'PISCO indist accuracy - spurious corr: ': 0.8029,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1579,\n",
       " 'PISCO ood accuracy- spurious corr: ': 0.6714,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8287,\n",
       " 'PISCO indist accuracy - no spurious corr: ': 0.8209,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6206,\n",
       " 'PISCO ood accuracy - no spurious corr: ': 0.6733}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import linalg as LA\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations\n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels \n",
    "    \n",
    "\n",
    "\n",
    "# call this function to get experiments results for different parameters\n",
    "def get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='resnet', transf_type='rotated', \n",
    "                    dataset='cifar10', eta=0.95):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load saved image features\n",
    "    z_train_og = load('./data/Z_train_og_cifar10_simclr.npy')\n",
    "    z_train_t = load('./data/Z_train_rotated_cifar10_simclr.npy')\n",
    "\n",
    "    z_test_og = load('./data/Z_test_og_cifar10_simclr.npy')\n",
    "    z_test_t = load('./data/Z_test_rotated_cifar10_simclr.npy')\n",
    "\n",
    "    y_train_og = load('./data/train_labels_cifar10.npy')\n",
    "\n",
    "    y_test_og = load('./data/test_labels_cifar10.npy')\n",
    "    \n",
    "    \n",
    "    # Create spurious correlations on train and test sets\n",
    "    z_train, train_labels = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_indist, indist_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_ood, ood_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= 1-alpha, binarize_label=False)\n",
    "  \n",
    "\n",
    "    # concatenate original and colored features\n",
    "    z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "    t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "    z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "    t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "    \n",
    "\n",
    "    # Obtain prediction coefficients of color\n",
    "    lr_model_t = LogisticRegression(random_state=0).fit(z_train_og_t, t_train_labels)\n",
    "    t_coefficients = lr_model_t.coef_.reshape(-1,1)\n",
    "    theta_1 = t_coefficients / np.linalg.norm(t_coefficients)\n",
    "\n",
    "    # Prediction Accuracies on image features extracted using a baseline model (mlp)\n",
    "    logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                                  random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "    baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "    baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "    baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)   \n",
    "    \n",
    "    # Trained on original baseline features, tested on colored features - no spurious correlations here\n",
    "    logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                     random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "    baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "    baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "    baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "\n",
    "    # Find P, get post-processed features, and perform predictions\n",
    "    k = int(z_train_og_t.shape[1]*eta) # % of original number of features\n",
    "    n = z_train_og_t.shape[0]\n",
    "\n",
    "    delta_z_matrix = z_train_og - z_train_t \n",
    "\n",
    "    M = - z_train_og_t.T @ z_train_og_t/n + lamda * delta_z_matrix.T @ delta_z_matrix / (n // 2 ) \n",
    "\n",
    "    # Performing SVD to get eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = LA.eigh(M)\n",
    "\n",
    "    # Forming P from eigenvectors and coeficients of color\n",
    "    P_1 = theta_1\n",
    "\n",
    "    P_2 = eigenvectors[:,:(k-1)]\n",
    "\n",
    "    P = np.concatenate((P_1,P_2), axis=1)\n",
    "    \n",
    "\n",
    "    # Obtain post-processed features\n",
    "    f_train_og = z_train_og @ P  \n",
    "    f_train = z_train @ P\n",
    "    f_test_indist = z_test_indist @ P\n",
    "    f_test_ood = z_test_ood @ P\n",
    "    f_test_og = z_test_og @ P\n",
    "    f_test_t = z_test_t @ P\n",
    "    f_test_og_t = z_test_og_t @ P\n",
    "    \n",
    "    \n",
    "\n",
    "    # Correlation Matrix Analysis\n",
    "    # concatenate transformation labels with f_test_og_t\n",
    "    t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "    t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "    corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "    corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "    corr_special = np.abs(corr_matrix[0,1])\n",
    "    corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "    z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "     \n",
    "     \n",
    "    \n",
    "\n",
    "    # Classification task using all post-processed features except style features   \n",
    "    lr_model_pisco_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                        random_state=0).fit(f_train[:,1:],train_labels)\n",
    "    pisco_sp_accuracy0 = lr_model_pisco_sp.score(f_train[:,1:], train_labels)\n",
    "    pisco_sp_accuracy1 = lr_model_pisco_sp.score(f_test_indist[:,1:], indist_test_labels)\n",
    "    pisco_sp_accuracy2 = lr_model_pisco_sp.score(f_test_ood[:,1:], ood_test_labels)\n",
    "    \n",
    "    # trained on original post-processed features, tested on transformed post-processed \n",
    "    # features without style features   \n",
    "    lr_model_pisco_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                        random_state=0).fit(f_train_og[:,1:],y_train_og)\n",
    "    pisco_no_sp_accuracy0 = lr_model_pisco_no_sp.score(f_train_og[:,1:], y_train_og)\n",
    "    pisco_no_sp_accuracy1 = lr_model_pisco_no_sp.score(f_test_og[:,1:], y_test_og)\n",
    "    pisco_no_sp_accuracy2 = lr_model_pisco_no_sp.score(f_test_t[:,1:], y_test_og)\n",
    "    \n",
    "    # put all the results in a dictionary\n",
    "    results_log = {}\n",
    "    results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "    results_log['PISCO indist accuracy - spurious corr: '] = pisco_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "    results_log['PISCO ood accuracy- spurious corr: '] = pisco_sp_accuracy2    \n",
    "\n",
    "    results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "    results_log['PISCO indist accuracy - no spurious corr: '] = pisco_no_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "    results_log['PISCO ood accuracy - no spurious corr: '] = pisco_no_sp_accuracy2   \n",
    "\n",
    "    \n",
    "    return results_log\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ITERS = range(10)\n",
    "#     datasets = ['mnist']\n",
    "#     extractors= ['mlp']\n",
    "#     transf_types = ['colored']\n",
    "#     alphas = [0.5, 0.75, 0.90, 0.95, 0.99,1.0]\n",
    "#     lamdas= [0,1,10,50] \n",
    "#     etas = [0.90]\n",
    "\n",
    "#     grid = list(product(datasets, extractors, transf_types, alphas, lamdas,etas,ITERS))\n",
    "    \n",
    "#     i = int(float(sys.argv[1]))\n",
    "#     dataset, extractor, transf_type, alpha, lamda, eta, ITER = grid[i]\n",
    "    \n",
    "\n",
    "#     results_log = get_exp_results(alpha = alpha, seed=ITER, lamda=lamda, extractor=extractor, \n",
    "#                                   transf_type=transf_type, dataset=dataset, eta=eta)\n",
    "\n",
    "#     with open(f'summary_mnist/summary_{i}.json', 'w') as fp:\n",
    "#         json.dump(results_log, fp)\n",
    "\n",
    "\n",
    "\n",
    "get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='simclr', transf_type='rotated', dataset='cifar10', eta=0.95) \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:invariance_env]",
   "language": "python",
   "name": "conda-env-invariance_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
