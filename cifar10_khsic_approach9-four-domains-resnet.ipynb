{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHSIC approach for disentangling content and style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution - column 0 is class labels - column 1 is domain/environment labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">7.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2\n",
       "1   0        \n",
       "0.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "1.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "2.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "3.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "4.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "5.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "6.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "7.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "8.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000\n",
       "9.0 0.0  5000\n",
       "    1.0  5000\n",
       "    2.0  5000\n",
       "    3.0  5000\n",
       "    4.0  5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/mctorch/nn/manifolds/stiefel.py:50: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1980.)\n",
      "  q, r = torch.qr(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.4844, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.2805, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.0809, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(0.0219, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.0313, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1061, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1272, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1286, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1298, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1733, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.1933, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.2061, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.2122, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.2281, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3044, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3463, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.3807, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4675, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4752, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4867, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.4875, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5692, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.5779, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6093, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6211, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  0\n",
      "loss:  tensor(-0.6243, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  1\n",
      "loss:  tensor(-0.6355, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  2\n",
      "loss:  tensor(-0.6424, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  3\n",
      "loss:  tensor(-0.6483, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  4\n",
      "loss:  tensor(-0.6534, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  5\n",
      "loss:  tensor(-0.6575, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  6\n",
      "loss:  tensor(-0.6609, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  7\n",
      "loss:  tensor(-0.6634, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  8\n",
      "loss:  tensor(-0.6653, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  9\n",
      "loss:  tensor(-0.6668, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  10\n",
      "loss:  tensor(-0.6679, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  11\n",
      "loss:  tensor(-0.6687, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  12\n",
      "loss:  tensor(-0.6693, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  13\n",
      "loss:  tensor(-0.6698, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  14\n",
      "loss:  tensor(-0.6701, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  15\n",
      "loss:  tensor(-0.6704, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  16\n",
      "loss:  tensor(-0.6706, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  17\n",
      "loss:  tensor(-0.6707, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  18\n",
      "loss:  tensor(-0.6708, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  19\n",
      "loss:  tensor(-0.6709, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  20\n",
      "loss:  tensor(-0.6709, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  21\n",
      "loss:  tensor(-0.6710, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  22\n",
      "loss:  tensor(-0.6710, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  23\n",
      "loss:  tensor(-0.6710, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  24\n",
      "loss:  tensor(-0.6711, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  25\n",
      "loss:  tensor(-0.6711, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  26\n",
      "loss:  tensor(-0.6711, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  27\n",
      "loss:  tensor(-0.6711, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  28\n",
      "loss:  tensor(-0.6712, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  29\n",
      "loss:  tensor(-0.6712, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  30\n",
      "loss:  tensor(-0.6712, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  31\n",
      "loss:  tensor(-0.6712, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  32\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  33\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  34\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  35\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  36\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  37\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  38\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  39\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  40\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  41\n",
      "loss:  tensor(-0.6713, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  42\n",
      "loss:  tensor(-0.6714, grad_fn=<SubBackward0>)\n",
      "Saving R, at epoch  43\n",
      "loss:  tensor(-0.6714, grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8d 42min 39s, sys: 7h 56min 56s, total: 8d 8h 39min 35s\n",
      "Wall time: 5h 33min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8952,\n",
       " 'HSIC indist accuracy - spurious corr: ': 0.864,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1079,\n",
       " 'HSIC ood accuracy- spurious corr: ': 0.1015,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8731,\n",
       " 'HSIC indist accuracy - no spurious corr: ': 0.8258,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6833,\n",
       " 'HSIC ood accuracy - no spurious corr: ': 0.5083}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = 1.0\n",
    "alpha_sk = 0.5 # for creating skewed data used to learn R\n",
    "eta = 0.95\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "ns = 4 #specify number of style features\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import linalg as LA\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import mctorch.nn as mnn\n",
    "import mctorch.optim as moptim\n",
    "from hsic_calculator import HSIC, normalized_HSIC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations  \n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    style_labels = np.concatenate((np.zeros(len(modified_images)), np.ones(len(unmodified_images))), axis=None)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels, style_labels.astype(int)\n",
    "    \n",
    "\n",
    "# call this function to get experiments results for different parameters    \n",
    "def get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='simclr', transf_type='contrasted', \n",
    "                    dataset='cifar10', eta=0.95):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load saved image features\n",
    "    z_train_og = load('./data/Z_train_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_train_t = load('./data/Z_train_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    z_test_og = load('./data/Z_test_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_test_t = load('./data/Z_test_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    y_train_og = load('./data/train_labels_'+dataset+'.npy')\n",
    "\n",
    "    y_test_og = load('./data/test_labels_'+dataset+'.npy')\n",
    "    \n",
    "    # Create spurious correlations on train and test sets\n",
    "    z_train, train_labels, _ = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_indist, indist_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_ood, ood_test_labels, _ = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= 1-alpha, binarize_label=False)\n",
    "   \n",
    "    # concatenate original and transformed features\n",
    "    z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "    t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "    z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "    t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "   \n",
    "    # Prediction Accuracies on image features extracted using a baseline model\n",
    "    logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                                  random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "    baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "    baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "    baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)\n",
    "    \n",
    "    # Trained on original baseline features, tested on transformed features - no spurious correlations here\n",
    "    logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                     random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "    baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "    baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "    baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "          \n",
    "    # Obtain prediction coefficients of transformations done on images\n",
    "    z_train_rotated = load('./data/Z_train_rotated_cifar10_'+extractor+'.npy')\n",
    "    z_train_contrasted = load('./data/Z_train_contrasted_cifar10_'+extractor+'.npy')\n",
    "    z_train_blurred = load('./data/Z_train_blurred_cifar10_'+extractor+'.npy')\n",
    "    z_train_saturated = load('./data/Z_train_saturated_cifar10_'+extractor+'.npy')\n",
    "       \n",
    "\n",
    "    # Find R, get post-processed features, and perform predictions\n",
    "    \n",
    "    z_train_og_4_ts = np.concatenate((z_train_og, z_train_rotated,z_train_contrasted, \n",
    "                                      z_train_blurred,z_train_saturated), axis=0)\n",
    "\n",
    "    og_4_ts_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_rotated)), \n",
    "                                     np.array([2]*len(z_train_contrasted)), np.array([3]*len(z_train_blurred)), \n",
    "                                     np.array([4]*len(z_train_saturated))), axis=None)\n",
    "    \n",
    "    \n",
    "    image_labels = np.concatenate((y_train_og, y_train_og, y_train_og, y_train_og, y_train_og), axis=None)\n",
    "    \n",
    "    # concatenate features with sytle labels..style labels are in column 0\n",
    "    og_4_ts_labels_z_train_og_4_ts = np.concatenate((og_4_ts_labels.reshape(-1,1), image_labels.reshape(-1,1), z_train_og_4_ts), axis=1)\n",
    "\n",
    "    # shuffle data in t_labels_z_train_og_t\n",
    "    np.random.shuffle(og_4_ts_labels_z_train_og_4_ts)\n",
    "\n",
    "    shuffled_train_og_t = og_4_ts_labels_z_train_og_4_ts[:,2:]\n",
    "    shuffled_t_train_labels = og_4_ts_labels_z_train_og_4_ts[:,:1]\n",
    "    \n",
    "    # class distribution \n",
    "    labels_and_z_train_df = pd.DataFrame(og_4_ts_labels_z_train_og_4_ts)\n",
    "\n",
    "    print(\"class distribution - column 1 is class labels - column 0 is domain/environment labels\")\n",
    "    class_distribution_per_domain = labels_and_z_train_df.groupby([1,0]).count().iloc[:,0:1]\n",
    "    display(class_distribution_per_domain)\n",
    "    \n",
    "    \n",
    "    dtype = torch.FloatTensor\n",
    "    n = shuffled_train_og_t.shape[0]\n",
    "    d = shuffled_train_og_t.shape[1]\n",
    "    k = int(shuffled_train_og_t.shape[1]*eta) # % of original number of features\n",
    "\n",
    "    # Initialize R\n",
    "    R = mnn.Parameter(manifold=mnn.Stiefel(d,k)).float()\n",
    "\n",
    "    # print(\"Initial R\")\n",
    "    # display(R)\n",
    "\n",
    "    # Define Objective function \n",
    "    def obj(z, e, W, n_s=1):\n",
    "        z = torch.from_numpy(z).float()\n",
    "        e = torch.from_numpy(e).float()\n",
    "        MI_content_style = normalized_HSIC(torch.matmul(z, W[:,:n_s]), torch.matmul(z, W[:,n_s:]))\n",
    "        MI_conten_env = normalized_HSIC(torch.matmul(z,W[:,n_s:]), e)\n",
    "        MI_style_env = normalized_HSIC(torch.matmul(z,W[:,:n_s]), e)\n",
    "        loss = (MI_content_style + MI_conten_env) - MI_style_env\n",
    "        return loss\n",
    "\n",
    "    # Optimize - passing data in mini-batches\n",
    "    optimizer = moptim.rAdagrad(params = [R], lr=1e-2)\n",
    "\n",
    "    best_loss = 1e5\n",
    "    checkpoint = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        for index in range(0, len(shuffled_train_og_t), batch_size):\n",
    "            train_data_subset = shuffled_train_og_t[index:index+batch_size]\n",
    "            style_labels_subset = shuffled_t_train_labels[index:index+batch_size]\n",
    "            loss = obj(train_data_subset, style_labels_subset, R, ns)        \n",
    "            # saving R with the smallest loss value so far\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                print(\"Saving R, at epoch \", epoch)\n",
    "                checkpoint = {'epoch': epoch, 'loss': loss, 'R': R}\n",
    "                torch.save(checkpoint, 'checkpoint') \n",
    "                print(\"loss: \", loss)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # Load saved R\n",
    "    R_mat = torch.load('checkpoint')['R']\n",
    "\n",
    "    # Obtain post-processed features\n",
    "    f_train_og = z_train_og @ R_mat.detach().numpy()  \n",
    "    f_train = z_train @ R_mat.detach().numpy()\n",
    "    f_test_indist = z_test_indist @ R_mat.detach().numpy()\n",
    "    f_test_ood = z_test_ood @ R_mat.detach().numpy()\n",
    "    f_test_og = z_test_og @ R_mat.detach().numpy()\n",
    "    f_test_t = z_test_t @ R_mat.detach().numpy()\n",
    "    f_test_og_t = z_test_og_t @ R_mat.detach().numpy()\n",
    "\n",
    "    \n",
    "    # Correlation Matrix Analysis\n",
    "    if transf_type=='rotated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,1])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "            \n",
    "        \n",
    "    elif transf_type=='contrasted':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,2])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "    \n",
    "        \n",
    "    elif transf_type=='blurred':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,3])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "        \n",
    "    elif transf_type=='saturated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,4])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "\n",
    "    # Classification task using all post-processed features except style features    \n",
    "    lr_model_hsic_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                        random_state=0).fit(f_train[:,4:],train_labels)\n",
    "    hsic_sp_accuracy0 = lr_model_hsic_sp.score(f_train[:,4:], train_labels)\n",
    "    hsic_sp_accuracy1 = lr_model_hsic_sp.score(f_test_indist[:,4:], indist_test_labels)\n",
    "    hsic_sp_accuracy2 = lr_model_hsic_sp.score(f_test_ood[:,4:], ood_test_labels)\n",
    "    \n",
    "    # trained on original post-processed features, tested on transformed post-processed features \n",
    "    # without features without style features  \n",
    "    lr_model_hsic_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                        random_state=0).fit(f_train_og[:,4:],y_train_og)\n",
    "    hsic_no_sp_accuracy0 = lr_model_hsic_no_sp.score(f_train_og[:,4:], y_train_og)\n",
    "    hsic_no_sp_accuracy1 = lr_model_hsic_no_sp.score(f_test_og[:,4:], y_test_og)\n",
    "    hsic_no_sp_accuracy2 = lr_model_hsic_no_sp.score(f_test_t[:,4:], y_test_og)\n",
    "    \n",
    "    # put all the results in a dictionary\n",
    "    results_log = {}\n",
    "    results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "    results_log['HSIC indist accuracy - spurious corr: '] = hsic_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "    results_log['HSIC ood accuracy- spurious corr: '] = hsic_sp_accuracy2    \n",
    "\n",
    "    results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "    results_log['HSIC indist accuracy - no spurious corr: '] = hsic_no_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "    results_log['HSIC ood accuracy - no spurious corr: '] = hsic_no_sp_accuracy2 \n",
    "    \n",
    "    return results_log\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ITERS = range(10)\n",
    "#     datasets = ['cifar10'] \n",
    "#     extractors= ['resnet', 'simclr']  \n",
    "#     transf_types = ['contrasted', 'rotated', 'blurred', 'saturated']  \n",
    "#     alphas = [0.5,0.75,0.90,0.95,0.99,1.0] \n",
    "#     lamdas= [0,1,10,50]\n",
    "#     etas = [0.90,0.93,0.95,0.98,1.0]\n",
    "\n",
    "#     grid = list(product(datasets, extractors, transf_types, alphas, lamdas,etas,ITERS))\n",
    "    \n",
    "#     i = int(float(sys.argv[1]))\n",
    "#     dataset, extractor, transf_type, alpha, lamda, eta, ITER = grid[i]    \n",
    "\n",
    "#     results_log = get_exp_results(alpha = alpha, seed=int(ITER), lamda=lamda, extractor=extractor, \n",
    "#                                   transf_type=transf_type, dataset=dataset, eta=eta)\n",
    "    \n",
    "#     with open(f'summary_cifar10/summary_{i}.json', 'w') as fp:\n",
    "#         json.dump(results_log, fp)\n",
    "\n",
    "\n",
    "get_exp_results(alpha = 1.0, seed=0, lamda=10, extractor='resnet', transf_type='rotated', dataset='cifar10', eta=0.95)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with PISCO Results on CIFAR 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/software/anaconda3/envs/invariance_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Baseline indist accuracy - spurious corr: ': 0.8952,\n",
       " 'PISCO indist accuracy - spurious corr: ': 0.8368,\n",
       " 'Baseline ood accuracy - spurious corr: ': 0.1079,\n",
       " 'PISCO ood accuracy- spurious corr: ': 0.6547,\n",
       " 'Baseline indist accuracy - no spurious corr: ': 0.8731,\n",
       " 'PISCO indist accuracy - no spurious corr: ': 0.8433,\n",
       " 'Baseline ood accuracy - no spurious corr: ': 0.6833,\n",
       " 'PISCO ood accuracy - no spurious corr: ': 0.7299}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import linalg as LA\n",
    "import torch\n",
    "from numpy import load\n",
    "import sys, json\n",
    "from itertools import product\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Function for binarizing labels\n",
    "def binarize(y):    \n",
    "    y = np.copy(y) > 5\n",
    "    return y.astype(int)\n",
    "\n",
    "# Function for creating spurious correlations\n",
    "def create_spurious_corr(z, z_t, y_og, spu_corr= 0.1, binarize_label=True):\n",
    "    y_bin = binarize(y_og)\n",
    "    mod_labels = np.logical_xor(y_bin, np.random.binomial(1, spu_corr, size=len(y_bin)))\n",
    "    \n",
    "    modified_images = z_t[mod_labels]\n",
    "    unmodified_images = z[~mod_labels]\n",
    "    all_z = np.concatenate((modified_images, unmodified_images), axis=0)\n",
    "    \n",
    "    all_img_labels = None\n",
    "    \n",
    "    if binarize_label:\n",
    "        modified_imgs_labels = y_bin[mod_labels]\n",
    "        unmodified_imgs_labels = y_bin[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)\n",
    "    else:\n",
    "        modified_imgs_labels = y_og[mod_labels]\n",
    "        unmodified_imgs_labels = y_og[~mod_labels]\n",
    "        all_img_labels = np.concatenate((modified_imgs_labels, unmodified_imgs_labels), axis=None)    \n",
    "        \n",
    "    return all_z, all_img_labels \n",
    "    \n",
    "\n",
    "# call this function to get experiments results for different parameters    \n",
    "def get_exp_results(alpha = 1.0, seed=0, lamda=1, extractor='simclr', transf_type='contrasted', \n",
    "                    dataset='cifar10', eta=0.95):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load saved image features\n",
    "    z_train_og = load('./data/Z_train_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_train_t = load('./data/Z_train_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    z_test_og = load('./data/Z_test_og_'+dataset+'_'+extractor+'.npy')\n",
    "    z_test_t = load('./data/Z_test_'+transf_type+'_'+dataset+'_'+extractor+'.npy')\n",
    "\n",
    "    y_train_og = load('./data/train_labels_'+dataset+'.npy')\n",
    "\n",
    "    y_test_og = load('./data/test_labels_'+dataset+'.npy')\n",
    "    \n",
    "    # Create spurious correlations on train and test sets\n",
    "    z_train, train_labels = create_spurious_corr(z_train_og, z_train_t, y_train_og, \n",
    "                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_indist, indist_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= alpha, binarize_label=False)\n",
    "\n",
    "    z_test_ood, ood_test_labels = create_spurious_corr(z_test_og, z_test_t, y_test_og, \n",
    "                                                             spu_corr= 1-alpha, binarize_label=False)\n",
    "   \n",
    "    # concatenate original and transformed features\n",
    "    z_train_og_t = np.concatenate((z_train_og, z_train_t), axis=0)\n",
    "    t_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_t))), axis=None) \n",
    "    z_test_og_t = np.concatenate((z_test_og, z_test_t), axis=0)\n",
    "    t_test_labels = np.concatenate((np.zeros(len(z_test_og)), np.ones(len(z_test_t))), axis=None) \n",
    "   \n",
    "    # Prediction Accuracies on image features extracted using a baseline model\n",
    "    logistic_regression_on_baseline = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                                  random_state=0).fit(z_train,train_labels)                                                                                     \n",
    "    baseline_accuracy0 = logistic_regression_on_baseline.score(z_train, train_labels)\n",
    "    baseline_accuracy1 = logistic_regression_on_baseline.score(z_test_indist, indist_test_labels)\n",
    "    baseline_accuracy2 = logistic_regression_on_baseline.score(z_test_ood, ood_test_labels)\n",
    "    \n",
    "    # Trained on original baseline features, tested on transformed features - no spurious correlations here\n",
    "    logistic_regression_on_baseline_og = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                                     random_state=0).fit(z_train_og,y_train_og)                                                                                     \n",
    "    baseline_og_accuracy0 = logistic_regression_on_baseline_og.score(z_train_og, y_train_og)\n",
    "    baseline_og_accuracy1 = logistic_regression_on_baseline_og.score(z_test_og, y_test_og)\n",
    "    baseline_transf_accuracy2 = logistic_regression_on_baseline_og.score(z_test_t, y_test_og)\n",
    "          \n",
    "    # Obtain prediction coefficients of transformations done on images\n",
    "    z_train_rotated = load('./data/Z_train_rotated_cifar10_'+extractor+'.npy')\n",
    "    z_train_contrasted = load('./data/Z_train_contrasted_cifar10_'+extractor+'.npy')\n",
    "    z_train_blurred = load('./data/Z_train_blurred_cifar10_'+extractor+'.npy')\n",
    "    z_train_saturated = load('./data/Z_train_saturated_cifar10_'+extractor+'.npy')\n",
    "       \n",
    "    z_train_og_rotated = np.concatenate((z_train_og, z_train_rotated), axis=0)\n",
    "    rotat_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_rotated))), axis=None)\n",
    "    \n",
    "    z_train_og_contrasted = np.concatenate((z_train_og, z_train_contrasted), axis=0)\n",
    "    contrast_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_contrasted))), axis=None)\n",
    "    \n",
    "    z_train_og_blurred= np.concatenate((z_train_og, z_train_blurred), axis=0)\n",
    "    blur_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_blurred))), axis=None)\n",
    "    \n",
    "    z_train_og_saturated = np.concatenate((z_train_og, z_train_saturated), axis=0)\n",
    "    saturat_train_labels = np.concatenate((np.zeros(len(z_train_og)), np.ones(len(z_train_saturated))), axis=None)\n",
    "    \n",
    "       \n",
    "    lr_model_rotated = LogisticRegression(random_state=0).fit(z_train_og_rotated, rotat_train_labels)\n",
    "    rotat_coefficients = lr_model_rotated.coef_.reshape(-1,1)\n",
    "    theta_1 = rotat_coefficients / np.linalg.norm(rotat_coefficients)\n",
    "    \n",
    "    lr_model_contrasted = LogisticRegression(random_state=0).fit(z_train_og_contrasted, contrast_train_labels)\n",
    "    contrast_coefficients = lr_model_contrasted.coef_.reshape(-1,1)\n",
    "    theta_2 = contrast_coefficients / np.linalg.norm(contrast_coefficients)\n",
    "    \n",
    "    lr_model_blurred = LogisticRegression(random_state=0).fit(z_train_og_blurred, blur_train_labels)\n",
    "    blur_coefficients = lr_model_blurred.coef_.reshape(-1,1)\n",
    "    theta_3 = blur_coefficients / np.linalg.norm(blur_coefficients)\n",
    "    \n",
    "    lr_model_saturated = LogisticRegression(random_state=0).fit(z_train_og_saturated, saturat_train_labels)\n",
    "    saturat_coefficients = lr_model_saturated.coef_.reshape(-1,1)\n",
    "    theta_4 = saturat_coefficients / np.linalg.norm(saturat_coefficients)\n",
    "       \n",
    "\n",
    "    # Find P, get post-processed features, and perform predictions\n",
    "    delta_z_matrix1 = z_train_og - z_train_rotated \n",
    "    delta_z_matrix2 = z_train_og - z_train_contrasted\n",
    "    delta_z_matrix3 = z_train_og - z_train_blurred\n",
    "    delta_z_matrix4 = z_train_og - z_train_saturated\n",
    "    combined_delta_z_matrix = np.concatenate((delta_z_matrix1, delta_z_matrix2,delta_z_matrix3,\n",
    "                                              delta_z_matrix4), axis=0)\n",
    "    \n",
    "    z_train_og_4_ts = np.concatenate((z_train_og, z_train_rotated,z_train_contrasted, \n",
    "                                      z_train_blurred,z_train_saturated), axis=0)\n",
    "    \n",
    "    k = int(z_train_og_4_ts.shape[1]*eta) # % of original number of features\n",
    "    n = z_train_og_4_ts.shape[0]\n",
    "    n_delt =  combined_delta_z_matrix.shape[0]\n",
    "\n",
    "    \n",
    "    M = - z_train_og_4_ts.T @ z_train_og_4_ts/n + lamda * combined_delta_z_matrix.T @ combined_delta_z_matrix /n_delt \n",
    "    \n",
    "    # Perform SVD to get eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = LA.eigh(M)\n",
    "\n",
    "    P_2 = eigenvectors[:,:(k-4)]\n",
    "\n",
    "    P = np.concatenate((theta_1,theta_2,theta_3,theta_4,P_2), axis=1)\n",
    "    \n",
    "    # Obtain post-processed features\n",
    "    f_train_og = z_train_og @ P  \n",
    "    f_train = z_train @ P \n",
    "    f_test_indist = z_test_indist @ P \n",
    "    f_test_ood = z_test_ood @ P \n",
    "    f_test_og = z_test_og @ P \n",
    "    f_test_t = z_test_t @ P \n",
    "    f_test_og_t = z_test_og_t @ P \n",
    "    \n",
    "    # Correlation Matrix Analysis\n",
    "    if transf_type=='rotated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,1])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "            \n",
    "        \n",
    "    elif transf_type=='contrasted':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,2])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "    \n",
    "        \n",
    "    elif transf_type=='blurred':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,3])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "        \n",
    "    elif transf_type=='saturated':\n",
    "        # concatenate transformation labels with f_test_og_t\n",
    "        t_labels_f_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), f_test_og_t), axis=1)\n",
    "        t_labels_z_test_og_t = np.concatenate((t_test_labels.reshape(-1,1), z_test_og_t), axis=1)\n",
    "        corr_matrix = np.corrcoef(t_labels_f_test_og_t.T)\n",
    "        corr_z_matrix = np.corrcoef(t_labels_z_test_og_t.T)\n",
    "        corr_special = np.abs(corr_matrix[0,4])\n",
    "        corr_ns_f_norm = np.sqrt((corr_matrix[0,5:]**2).mean()) \n",
    "        z_corr_ns_f_norm = np.sqrt((corr_z_matrix[0,:]**2).mean()) \n",
    "        \n",
    "\n",
    "    # Classification task using all post-processed features except style features    \n",
    "    lr_model_pisco_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                                        random_state=0).fit(f_train[:,4:],train_labels)\n",
    "    pisco_sp_accuracy0 = lr_model_pisco_sp.score(f_train[:,4:], train_labels)\n",
    "    pisco_sp_accuracy1 = lr_model_pisco_sp.score(f_test_indist[:,4:], indist_test_labels)\n",
    "    pisco_sp_accuracy2 = lr_model_pisco_sp.score(f_test_ood[:,4:], ood_test_labels)\n",
    "    \n",
    "    # trained on original post-processed features, tested on transformed post-processed features \n",
    "    # without features without style features  \n",
    "    lr_model_pisco_no_sp = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                        random_state=0).fit(f_train_og[:,4:],y_train_og)\n",
    "    pisco_no_sp_accuracy0 = lr_model_pisco_no_sp.score(f_train_og[:,4:], y_train_og)\n",
    "    pisco_no_sp_accuracy1 = lr_model_pisco_no_sp.score(f_test_og[:,4:], y_test_og)\n",
    "    pisco_no_sp_accuracy2 = lr_model_pisco_no_sp.score(f_test_t[:,4:], y_test_og)\n",
    "    \n",
    "    # put all the results in a dictionary\n",
    "    results_log = {}\n",
    "    results_log['Baseline indist accuracy - spurious corr: '] = baseline_accuracy1\n",
    "    results_log['PISCO indist accuracy - spurious corr: '] = pisco_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - spurious corr: '] = baseline_accuracy2 \n",
    "    results_log['PISCO ood accuracy- spurious corr: '] = pisco_sp_accuracy2    \n",
    "\n",
    "    results_log['Baseline indist accuracy - no spurious corr: '] = baseline_og_accuracy1\n",
    "    results_log['PISCO indist accuracy - no spurious corr: '] = pisco_no_sp_accuracy1\n",
    "\n",
    "    results_log['Baseline ood accuracy - no spurious corr: '] = baseline_transf_accuracy2            \n",
    "    results_log['PISCO ood accuracy - no spurious corr: '] = pisco_no_sp_accuracy2 \n",
    "    \n",
    "    return results_log\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ITERS = range(10)\n",
    "#     datasets = ['cifar10'] \n",
    "#     extractors= ['resnet', 'simclr']  \n",
    "#     transf_types = ['contrasted', 'rotated', 'blurred', 'saturated']  \n",
    "#     alphas = [0.5,0.75,0.90,0.95,0.99,1.0] \n",
    "#     lamdas= [0,1,10,50]\n",
    "#     etas = [0.90,0.93,0.95,0.98,1.0]\n",
    "\n",
    "#     grid = list(product(datasets, extractors, transf_types, alphas, lamdas,etas,ITERS))\n",
    "    \n",
    "#     i = int(float(sys.argv[1]))\n",
    "#     dataset, extractor, transf_type, alpha, lamda, eta, ITER = grid[i]    \n",
    "\n",
    "#     results_log = get_exp_results(alpha = alpha, seed=int(ITER), lamda=lamda, extractor=extractor, \n",
    "#                                   transf_type=transf_type, dataset=dataset, eta=eta)\n",
    "    \n",
    "#     with open(f'summary_cifar10/summary_{i}.json', 'w') as fp:\n",
    "#         json.dump(results_log, fp)\n",
    "\n",
    "\n",
    "get_exp_results(alpha = 1.0, seed=0, lamda=10, extractor='resnet', transf_type='rotated', dataset='cifar10', eta=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:invariance_env]",
   "language": "python",
   "name": "conda-env-invariance_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
